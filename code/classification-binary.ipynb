{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc678e1-3c64-4404-8bf4-45d68e07e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f6196-b711-433b-9b1a-554164e9e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea94a5-7676-494b-9d92-0373d18b7f10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da63a0-c0ed-429d-b298-f2cf436e4e68",
   "metadata": {},
   "source": [
    "Fetal Health Classification Dataset: https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification\n",
    "\n",
    "Original article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822315/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3c47d-f97e-424c-a786-d34cb7e43097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', 'fetal_health.csv'))\n",
    "df_input.columns = [col.replace('_', ' ') for col in df_input.columns]\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcb9ae-8015-4e60-92b2-6a18a5bddb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa18ae-5008-4a3a-ae9b-b720cbcfe4f2",
   "metadata": {},
   "source": [
    "Fetal health (target column):\n",
    "- 1 - Normal\n",
    "- 2 - Suspect\n",
    "- 3 - Pathological\n",
    "\n",
    "For this notebook, we will consider only the Normal/not Normal distinction (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7879d-d3ac-4085-9a26-d4b25d46eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input['fetal health'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6559e-6643-47ef-949c-a5c654234a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input['fetal health binary'] = (df_input['fetal health'] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33551913-38db-4627-8746-b57587116c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input['fetal health binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5af30-17dc-4542-bcc0-a1c9886e6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_input[[col for col in df_input.columns if col not in ['fetal health', 'fetal health binary']]]\n",
    "y = df_input['fetal health binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44e3db-35e5-4367-9c01-16a529aad418",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = {0:'Not Normal', 1:'Normal'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585be284-1269-47e4-91b3-559418ee207e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87ce4d-ad2a-47de-9c8d-efdf8819d82c",
   "metadata": {},
   "source": [
    "Illustrative EDA example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed020b6-401a-4a89-afb9-f0ec41de043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 24))\n",
    "\n",
    "plot_cols = [col for col in X.columns if not col.startswith('histogram')]\n",
    "pairplot = sns.pairplot(pd.concat([X[plot_cols], y], axis=1), hue='fetal health binary')\n",
    "# rotate y labels\n",
    "for ax in pairplot.axes.flatten():\n",
    "    # rotate x axis labels\n",
    "    ax.set_xlabel(ax.get_xlabel(), rotation=0)\n",
    "    # rotate y axis labels\n",
    "    ax.set_ylabel(ax.get_ylabel(), rotation=80)\n",
    "    # set y labels alignment\n",
    "    ax.yaxis.get_label().set_horizontalalignment('right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee724a2-239d-4b40-b736-81036be5dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(X, y, features_to_plot):\n",
    "    \n",
    "    fig, axs = plt.subplots(math.ceil(len(features_to_plot)/PLOTS_PER_ROW),PLOTS_PER_ROW, figsize=(22, 4*math.ceil(len(features_to_plot)/3)))\n",
    "    fig.subplots_adjust(hspace=.3, wspace=.2)\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    PLOTS_PER_ROW = 4\n",
    "    for feature in features_to_plot:\n",
    "        sns.boxplot(\n",
    "            x=y.apply(lambda cell: classes_dict[cell]), y=X[feature], order=classes_dict.values(), ax=axs[i][j], palette=\"YlGnBu\",\n",
    "            medianprops=dict(linewidth=4, color=\"blue\", alpha=1.0), flierprops=dict(markerfacecolor=\"#707070\", marker=\"d\"),\n",
    "            showmeans=True, meanprops={\"marker\":\"X\",\"markerfacecolor\":\"limegreen\", \"markeredgecolor\":\"green\" ,\"markersize\":10}\n",
    "        )\n",
    "        axs[i][j].set_title(feature, fontsize=MEDIUM_SIZE)\n",
    "        axs[i][j].set_ylabel('', fontsize=SMALL_SIZE)\n",
    "        axs[i][j].set_xlabel('', fontsize=SMALL_SIZE)\n",
    "\n",
    "        j += 1\n",
    "        if j % PLOTS_PER_ROW == 0:\n",
    "            i += 1\n",
    "            j = 0\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb9a15-1ef0-4693-ad5d-3b9dedcf6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(X, y, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bab256-5d73-4c6f-9955-6c5882236837",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b435a4-ca55-49d7-b02c-43292d178f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Pearson's Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f8bd9-dab7-4084-9b7b-1c02b3d727f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb7ccd-aac1-483a-bf15-dcacb86e4e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute features' correlation\n",
    "X_corr = X.corr()\n",
    "\n",
    "# Generate a mask to onlyshow the bottom triangle\n",
    "mask_corr = np.triu(np.ones_like(X_corr, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fde340-754f-4d45-8c49-6da4313d823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.title(\"Features' Pearson's Correlation Coefficient\", fontsize=18)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# generate heatmap\n",
    "sns.heatmap(X_corr, cmap=\"YlGnBu\", annot=True, mask=mask_corr, vmin=-1, vmax=1, square=True, annot_kws={\"fontsize\":8}, fmt=\".2f\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d263ba-102c-4621-a730-c7149c9c3768",
   "metadata": {},
   "source": [
    "Most features do not appear to have strong positive or negative correlations with other features, but there are cases where correlation is high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96751170-f284-474c-bedf-c7fcfd936ae5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Multicollinearity\n",
    "\n",
    "> Multicollinearity is a problem because it undermines the statistical significance of an independent variable. [\\[source\\]](https://link.springer.com/chapter/10.1007/978-0-585-25657-3_37)\n",
    "\n",
    "> Multicollinearity does not affect the accuracy of predictive models, including regression models. \\[...\\] Now, where multicollinearity becomes 'an issue' is when you want to 'interpret' the parameters learned by your model. In other words, you cannot say that the feature with the 'biggest weight' is 'the most important' when the features are correlated. Note that this is independent on the accuracy of the model, this is only the interpretation part [\\[source\\]](https://www.researchgate.net/post/Are-Random-Forests-affected-by-multi-collinearity-between-features)\n",
    "\n",
    "> Multicollinearity is only a problem for inference in statistics and analysis. For example, if youâ€™d like to infer the importance of certain features, then almost by definition multicollinearity means that some features are shown as strongly/perfectly correlated with other combination of features, and therefore they are undistinguishable. In this case, you can simply remove the problematic features.\n",
    "Multicollinearity is not a real problem for prediction. GBT, being more of a black-box model, is more suitable for predictions problems to start with, although of course you could try and use them for inference as well.\n",
    "[\\[source\\]](https://www.quora.com/Is-multicollinearity-a-problem-with-gradient-boosted-trees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67960bd4-97ff-4527-9c58-4f357fadca15",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.1. Variance Inflation Factor (VIF)\n",
    "\n",
    "Hereâ€™s the formula for calculating the VIF for feature X1: $VIF_{1}=\\frac{1}{1-R_{1}^{2}}$\n",
    "\n",
    "$R^2$ in this formula is the coefficient of determination from the linear regression model which has: X1 as dependent variable (target), and X2, X3, ... as independent variables (features). To calculate VIF for each features, you have to fit a linear regression using such feature as the target.\n",
    "\n",
    "As a general rule of thumb, \"VIF > 5 is cause for concern and VIF > 10 indicates a serious collinearity problem.\"\n",
    "\n",
    "- If feature X1 has VIF = 1 (minimum possible value for VIF), then there is zero collinearity between this feature and the other features in the dataset (e.g. X2, X3, ...)\n",
    "- If feature X1 has VIF = 2.5, then the variance of the regression coefficient of X1 in the original linear regression model is 2.5 times greater than it would have been if X1 had been entirely non-related to other features\n",
    "- If feature X1 has VIF = Inf, then X1 can be perfectly predicted by using the other features in the dataset\n",
    "[\\[source\\]](https://quantifyinghealth.com/vif-threshold/#:~:text=Most%20research%20papers%20consider%20a,of%205%20or%20even%202.5.)\n",
    "\n",
    "The higher the VIF:\n",
    "- The more correlated a predictor is with the other predictors\n",
    "- The more the standard error is inflated\n",
    "- The larger the confidence interval\n",
    "- The less likely it is that a coefficient will be evaluated as statistically significant\n",
    "[\\[source\\]](https://towardsdatascience.com/everything-you-need-to-know-about-multicollinearity-2f21f082d6dc)\n",
    "\n",
    "For implementation example with evaluation on performance and feature importance, read [\\[this article\\]](https://towardsdatascience.com/targeting-multicollinearity-with-python-3bd3b4088d0b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377dda9-5217-4735-97ba-baf1fba10dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67472b38-6ec1-4bb8-8d5c-29f374bd81e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_vif = pd.DataFrame(X.columns, columns=['feature'])\n",
    "df_vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "df_vif = df_vif.sort_values('VIF', ascending=False)\n",
    "df_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07fc18-0f5f-4b66-ac01-85eb5b1de08f",
   "metadata": {},
   "source": [
    "**Which features would remain if we removed all features with VIF > 10?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85092653-18a9-4903-a52c-0313a3304add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vif(X, threshhold=10):\n",
    "    features = list(X.columns)\n",
    "    max_len = max([len(f) for f in features])\n",
    "    high_vif_cols = []\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        vif = [variance_inflation_factor(X[features].values, ix) for ix in range(X[features].shape[1])]\n",
    "        max_idx = np.argmax(vif)\n",
    "        if max(vif) > threshhold:\n",
    "            high_vif_col = features.pop(max_idx)\n",
    "            high_vif_cols.append(high_vif_col)\n",
    "            print(f'{i+1}. Removed feature: {(high_vif_col+\" \").ljust(max_len+2,\".\")} VIF: {vif[max_idx]:.2f}')\n",
    "\n",
    "    print(f'\\nRemaining features:\\n{features}')\n",
    "    \n",
    "    return high_vif_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa282d94-dd8a-4688-bbe1-88e9d0e7f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vif_cols = check_vif(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c50037-1c9a-455b-b7b5-5043d9cb4d6f",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5d24b-e077-4221-a54f-128ad3b1c391",
   "metadata": {},
   "source": [
    "#### 2.2.2. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2edee7-93c0-4ac0-9198-39e49ef5933c",
   "metadata": {},
   "source": [
    "Principal Components Analysis (PCA) is a well-known unsupervised dimensionality reduction technique that constructs relevant features/variables through linear (linear PCA) or non-linear (kernel PCA) combinations of the original variables (features).\n",
    "\n",
    "The construction of relevant features is achieved by linearly transforming correlated variables into a smaller number of uncorrelated variables. This is done by projecting (dot product) the original data into the reduced PCA space using the eigenvectors of the covariance/correlation matrix aka the principal components (PCs).\n",
    "\n",
    "The resulting projected data are essentially linear combinations of the original data capturing most of the variance in the data.\n",
    "\n",
    "[\\[source\\]](https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e)\n",
    "\n",
    "Additional insighful article on multicollinearity and model interpretability: [\\[source\\]](https://marinawyss.github.io/multicollinearity/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed5908-5c05-437a-bfb7-2c29532bc30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab7391-7adc-4e9c-9bf3-5d3224c044bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X\n",
    "stdscaler = StandardScaler()\n",
    "X_standardized = pd.DataFrame(stdscaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1598b6-46cf-4288-9c63-9f9c95d1bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all possible PC's\n",
    "pca_initial = PCA(n_components=X_standardized.shape[1])\n",
    "\n",
    "pca_cols = [f'PC{i}' for i in range(1,X_standardized.shape[1]+1)]\n",
    "# The PCA model requires standardized (z-scored) data\n",
    "X_pca = pd.DataFrame(pca_initial.fit_transform(X_standardized), columns=pca_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f47285-99f8-4bde-b808-4c897e11d70c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_vif_pca = pd.DataFrame(pca_cols, columns=['PC'])\n",
    "df_vif_pca['VIF'] = [variance_inflation_factor(X_pca, i) for i in range(X_pca.shape[1])]\n",
    "df_vif_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5d179-4115-4f79-a968-ac4fe94141e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vif_pca['VIF'].round(decimals=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116d09f-a547-477e-b6d5-cbfb59e250ca",
   "metadata": {},
   "source": [
    "With PCA, we removed all the multicolinearity in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074580d-9a3a-4cf2-a50f-26a3a62bf062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute features' correlation\n",
    "X_corr_pca = X_pca.corr()\n",
    "\n",
    "# Generate a mask to onlyshow the bottom triangle\n",
    "mask_corr_pca = np.triu(np.ones_like(X_corr_pca, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09698703-c394-419a-b5d6-d4bb41078a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.title(\"Features' Pearson's Correlation Coefficient\", fontsize=18)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# generate heatmap\n",
    "sns.heatmap(X_corr_pca, cmap=\"YlGnBu\", annot=True, mask=mask_corr_pca, vmin=-1, vmax=1, square=True, annot_kws={\"fontsize\":8}, fmt=\".2f\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633f585-fe52-428a-a5ed-ff1a32ff283a",
   "metadata": {},
   "source": [
    "#### 2.2.3. PCA after train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208047c-0d51-4359-a0c9-54827fda2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333b377-8105-4a2d-bd49-5b67a2174f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, df_input.index, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e375d-de34-4362-9f27-fdfe025ab6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432955d0-ae9c-4e1b-8640-d73b3a9d57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X_train\n",
    "stdscaler = StandardScaler()\n",
    "X_train_std = pd.DataFrame(stdscaler.fit_transform(X_train), columns=X.columns, index=idx_train)\n",
    "X_test_std = pd.DataFrame(stdscaler.transform(X_test), columns=X.columns, index=idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba28c6-3bb3-49c4-9f14-1f4c71619001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Select components that explain 90% of the variability in the original data\n",
    "n_components = np.argmax(pca_initial.explained_variance_ratio_.cumsum() > 0.98) + 1\n",
    "# B) use all PC's\n",
    "# n_components = X.shape[1]\n",
    "\n",
    "# use selected PC's\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "pca_cols = [f'PC{i}' for i in range(1,n_components+1)]\n",
    "# The PCA model requires standardized (z-scored) data\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_std), columns=pca_cols, index=idx_train)\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test_std), columns=pca_cols, index=idx_test)\n",
    "X_train_pca_coeffs = pd.DataFrame(pca.components_, index=pca_cols, columns=X.columns)\n",
    "\n",
    "print(f\"Number of Principal Components (PC's) used: {len(pca.explained_variance_ratio_)} (out of {X.shape[1]} possible PC's)\")\n",
    "print(f\"Those {len(pca.explained_variance_ratio_)} PC's explain {pca.explained_variance_ratio_.sum()*100:.1f}% of the variance in the original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95e10b-ca59-46f0-a0a2-746ab38cd2ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auxiliar Functions' Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d6bd2-0eed-43cc-93ba-48348fa7c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a89726-3f8e-4380-951b-2e38445be1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "    \n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=BIGGER_SIZE, pad=20)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, ['\\n'.join(name.rsplit(' ')) for name in target_names], rotation=0, fontsize=MEDIUM_SIZE)\n",
    "        plt.yticks(tick_marks, ['\\n'.join(name.rsplit(' ')) for name in target_names], fontsize=MEDIUM_SIZE)\n",
    "        \n",
    "    cm_copy = cm.copy()\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm_copy.max() / 1.5 if normalize else cm_copy.max() / 2\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.1f}%\\n({})\".format(cm[i, j]*100, cm_copy[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm_copy[i, j] > thresh else \"black\",\n",
    "                     fontsize=MEDIUM_SIZE)\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm_copy[i, j] > thresh else \"black\",\n",
    "                     fontsize=MEDIUM_SIZE)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=MEDIUM_SIZE, labelpad=10)\n",
    "    plt.xlabel('Predicted label\\n\\naccuracy={:.2f}%; misclassification={:.2f}%'.format(100*accuracy, 100*misclass), fontsize=MEDIUM_SIZE, labelpad=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d6315-7e93-405b-b678-3404eb825b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_cv(X_train_, y_train_, model, space, n_splits, n_repeats,\n",
    "                  scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'], refit='f1', random_state=0):\n",
    "    # ignore warnings via env var because GridSearchCV with n_jobs=-1 triggers parallel backend warnings\n",
    "    os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "    # define evaluation\n",
    "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "    # define search\n",
    "    search = GridSearchCV(model, space, scoring=scoring, n_jobs=-1, cv=cv, refit=refit)\n",
    "    # execute search\n",
    "    result_cv = search.fit(X_train_, y_train_)\n",
    "    \n",
    "    return result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459a41c-f25b-4d8f-873b-e5f08bb3a582",
   "metadata": {},
   "source": [
    "Function to calculate p-values for scikit learn Logistic Regression model.\n",
    "\n",
    "Source: https://stackoverflow.com/a/47079198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df5f77-eec1-41be-a8cb-e843ec609e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance/47079198#47079198\n",
    "from scipy.stats import norm\n",
    "\n",
    "def logit_stderror(model, x):\n",
    "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "    parameters:\n",
    "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "        x:     matrix on which the model was fit\n",
    "    This function uses asymtptics for maximum likelihood estimates.\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se\n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    \n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f085f6-b252-48ac-98d2-88d7f23c7db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_importance(df_importance, title, error=None):\n",
    "\n",
    "    \n",
    "    df_importance['Absolute Coefficients'] = df_importance['Coefficients'].abs()\n",
    "    df_importance = df_importance.sort_values('Absolute Coefficients', ascending=True)\n",
    "\n",
    "    colors_dict = {\n",
    "        'Positive': 'royalblue',\n",
    "        'Negative': 'firebrick'\n",
    "    }\n",
    "    df_importance['Color'] = df_importance.apply(\n",
    "        lambda row: colors_dict['Negative'] if row['Coefficients'] < 0 else colors_dict['Positive'], axis=1\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    max_coeff = df_importance['Absolute Coefficients'].max()\n",
    "    err_max_coeff = error[np.argmax(df_importance['Absolute Coefficients'].max())]\n",
    "    big_errors = []\n",
    "    for i, err in enumerate(error):\n",
    "        if err > max_coeff + err_max_coeff:\n",
    "            big_errors.append((i, err))\n",
    "            error[i] = max_coeff\n",
    "            \n",
    "    for idx_err, big_err in big_errors:\n",
    "        display_string = f\"  ...  (Margin of Error: {big_err:,.2f})\"\n",
    "        ax.text(df_importance['Absolute Coefficients'].iloc[idx_err] + max_coeff, idx_err-0.025, display_string, color='black', fontweight='bold')\n",
    "\n",
    "    df_importance['Absolute Coefficients'].plot(\n",
    "        kind=\"barh\", color=df_importance['Color'], figsize=(10, df_importance.shape[0]/2), legend=False, ax=ax, \n",
    "        xerr=error, ecolor='black', error_kw={'label':'95% confidence interval', 'capsize':4, 'capthick':1}\n",
    "    )\n",
    "    \n",
    "    ax.xaxis.grid()\n",
    "    ax.set_axisbelow(True)\n",
    "    plt.title(title, pad=20, fontsize=BIGGER_SIZE)\n",
    "    legend_patches = [\n",
    "        ax.get_legend_handles_labels()[0][0], # confidence interval\n",
    "        mpatches.Patch(color=colors_dict['Positive'], label='Positive Coefficient'),\n",
    "        mpatches.Patch(color=colors_dict['Negative'], label='Negative Coefficient'),\n",
    "    ]\n",
    "    plt.legend(handles=legend_patches, loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "    \n",
    "    return df_importance[['Coefficients', 'Absolute Coefficients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6a8aa-13fa-4944-9f9c-c072dca50863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_cv(result_cv):\n",
    "    print('Grid Search CV Best Model - Scoring Metrics:\\n')\n",
    "    print(f\"ROC AUC:   {result_cv.cv_results_['mean_test_roc_auc'][result_cv.best_index_]:.3f}\")\n",
    "    print(f\"Accuracy:  {result_cv.cv_results_['mean_test_accuracy'][result_cv.best_index_]:.3f}\")\n",
    "    print(f\"Precision: {result_cv.cv_results_['mean_test_precision'][result_cv.best_index_]:.3f}\")\n",
    "    print(f\"Recall:    {result_cv.cv_results_['mean_test_recall'][result_cv.best_index_]:.3f}\")\n",
    "    print(f\"F1 score:  {result_cv.cv_results_['mean_test_f1'][result_cv.best_index_]:.3f}\\n\")\n",
    "\n",
    "    print(f'\\nBest Hyperparameters: {result_cv.best_params_}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a909b-2002-44ee-a613-f48a2d7cf728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test_, y_pred_):\n",
    "    print('Final Model - Scoring Metrics on Test Dataset:\\n')\n",
    "    print(f\"ROC AUC:   {roc_auc_score(y_test_, y_pred_):.3f}\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test_, y_pred_):.3f}\")\n",
    "    print(f\"Precision: {precision_score(y_test_, y_pred_):.3f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test_, y_pred_):.3f}\")\n",
    "    print(f\"F1 score:  {f1_score(y_test_, y_pred_):.3f}\\n\\n\")\n",
    "    print('Classification Report: \\n')\n",
    "    print(classification_report(y_test_, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a87662-0e39-4269-9ebe-2171d84cac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod import families\n",
    "\n",
    "def box_tidwell_test(X, continuous_features):\n",
    "\n",
    "    cols_to_keep = []\n",
    "    X_copy = X.copy()\n",
    "\n",
    "    # Add logit transform interaction terms (natural log) for continuous variables e.g.. Age * Log(Age)\n",
    "    for var in continuous_features:\n",
    "        log_var = f'log({var})'\n",
    "        cols_to_keep.append(var)\n",
    "        cols_to_keep.append(log_var)\n",
    "        X_copy[log_var] = X_copy[var].apply(lambda x: x * np.log(x))\n",
    "\n",
    "    # keep only numeric columns and their corresponding log columns\n",
    "    X_copy = X_copy[cols_to_keep]\n",
    "    X_copy_cte = sm.add_constant(X_copy, prepend=False)\n",
    "\n",
    "\n",
    "    # Building model and fit the data (using statsmodel's Logit)\n",
    "    logit_results = GLM(y, X_copy_cte, family=families.Binomial()).fit()\n",
    "\n",
    "    # Display summary results\n",
    "    # print(logit_results.summary())\n",
    "\n",
    "    results_as_html = logit_results.summary().tables[1].as_html()\n",
    "    df_summary = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "\n",
    "    features_with_nonlinearity = []\n",
    "    for idx, row in df_summary.iterrows():\n",
    "        if idx[:3] == 'log' and row['P>|z|'] <= 0.05:\n",
    "            features_with_nonlinearity.append(idx.split(')')[0].split('(')[1])\n",
    "\n",
    "    print('Features with non-linear relashionship with the log-odds,')\n",
    "    print('i.e. log(feature) is statistically significant (p-value<0.05):\\n')\n",
    "    for i, feat in enumerate(features_with_nonlinearity):\n",
    "        print(f' {i+1}. {feat}')\n",
    "    print('\\nConsider performing higher-order polynomial transformations to capture the non-linearity (e.g., featureÂ²).')\n",
    "    \n",
    "    return features_with_nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffd9a5-8ba5-480b-ae31-6e361370bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def plot_log_odds_graphs(X, features_to_plot):\n",
    "    \n",
    "    # Add constant\n",
    "    X_cte = sm.add_constant(X, prepend=False)\n",
    "    # Re-run logistic regression on original set of X and y variables\n",
    "    logit_results = GLM(y, X_cte, family=families.Binomial()).fit()\n",
    "    predicted = logit_results.predict(X_cte)\n",
    "\n",
    "    # Get log odds values\n",
    "    log_odds = np.log(predicted / (1 - predicted))\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    PLOTS_PER_ROW = 4\n",
    "    fig, axs = plt.subplots(math.ceil(len(features_to_plot)/PLOTS_PER_ROW),PLOTS_PER_ROW, figsize=(22, 5*math.ceil(len(features_to_plot)/3)))\n",
    "    fig.subplots_adjust(hspace=.2, wspace=.15)\n",
    "    for feature in features_to_plot:\n",
    "        axs[i][j].scatter(x=X_cte[feature].values, y=log_odds, s=3)\n",
    "        axs[i][j].set_xlabel(feature)\n",
    "        if j == 0:\n",
    "            axs[i][j].set_ylabel('Log-odds')\n",
    "        j += 1\n",
    "        if j % PLOTS_PER_ROW == 0:\n",
    "            i += 1\n",
    "            j = 0\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84814b1a-ae3e-4ea8-a8c2-ed3ecb645bc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f88caf-4523-46fc-9178-f18fa07c3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93075506-f6f8-41ee-b62e-6c886fe737e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d20e0b-358d-449a-b25f-38380e8a4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_logreg = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'C': np.logspace(-5, 2, num=15, base=10.0), # 10e-5 to 100 in 15 steps\n",
    "    'class_weight': ['balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5f915-3025-45ff-aad9-ed43f35effb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Check for Linearity between independent variables and the log-odds of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d12c0-6ca6-4bba-8081-c92b0d7bc667",
   "metadata": {},
   "source": [
    "The logistic regression model assumes a linear relationship between each independent variable and the logit (aka log-odds) of the outcome.\n",
    "\n",
    "Explanation of Logistic Regression assumptions: https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa2f86-1eb1-4aab-8dad-67ba055cabce",
   "metadata": {},
   "source": [
    "**A) Box-Tidwell Test** (only works with rows with positive values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01621466-1359-49d2-ab55-300b81540244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_with_nonlinearity = box_tidwell_test(X, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fddddc-d073-4178-9a13-99abf40fc3f1",
   "metadata": {},
   "source": [
    "**B) Visual Inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121557eb-76d1-4e2c-bf27-b7171a2be17b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log_odds_graphs(X, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b8932-28e1-4864-86eb-31959034d2c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1. Logistic Regression on PC's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f08075-aa49-49fd-a016-3ac30ac7b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Hyperparameter tuning with Grid Search Cross Validation\n",
    "\n",
    "result_cv_logreg_pca = gridsearch_cv(X_train_pca, y_train, model=LogisticRegression(), \n",
    "                                 space=space_logreg, n_splits=4, n_repeats=2, refit='f1', random_state=0)\n",
    "result_cv_logreg_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd834a-931a-4dc0-86f0-a2248e983623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_cv(result_cv_logreg_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a3e43-0c04-4f4f-9e86-27b1155709cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define best model\n",
    "model_logreg_pca = LogisticRegression(\n",
    "    **result_cv_logreg_pca.best_params_, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf488f9-a79a-4381-be5e-4f0690a3a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and make predictions\n",
    "model_logreg_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model_logreg_pca.predict(X_test_pca)\n",
    "y_pred_proba_pca = model_logreg_pca.predict_proba(X_test_pca)\n",
    "print(f'Accuracy on Training Set: {100*model_logreg_pca.score(X_train_pca, y_train):.1f}%')\n",
    "print(f'Accuracy on Test Set:     {100*model_logreg_pca.score(X_test_pca, y_test):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021ccf0-be3e-4ac6-9ed9-369ae1e4e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb532624-b58e-4b76-a875-03d537a54f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_pca = confusion_matrix(y_test, y_pred_pca)\n",
    "plot_confusion_matrix(cm_pca, target_names=[classes_dict[i] for i in model_logreg_pca.classes_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b40a6-7149-49dd-b25b-7fdc8df04ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ee15f-f76d-4a54-b576-f4470a79b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_components_pca = pd.DataFrame(np.transpose(model_logreg_pca.coef_), columns=['Coefficients'], index=pca_cols)\n",
    "stderror_components = logit_stderror(model_logreg_pca, X_train_pca)[1:]\n",
    "df_importance_components_pca = plot_importance(df_importance_components_pca, title=\"Logistic Regression on PC's - Absolute Coefficient Values with 95% CI\", \n",
    "                                               error=1.96*stderror_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d562c6-3a04-42dd-9a11-d2c3d6839c86",
   "metadata": {},
   "source": [
    "CI for the coefficients of the original features is the square root of the sum of the squares of the radius of the CI for the coefficients of the principal components.\n",
    "\n",
    "Sources:\n",
    "- Stack Exchange: https://stats.stackexchange.com/a/224760\n",
    "- Uncertainties and Error Propagation: https://www.geol.lsu.edu/jlorenzo/geophysics/uncertainties/Uncertaintiespart2.html\n",
    "- Error Propagation (Propagation of Uncertainty): https://www.statisticshowto.com/statistics-basics/error-propagation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057eb61-7425-4061-9681-2a08f3b3364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_pca = X_train_pca_coeffs.multiply(\n",
    "                        df_importance_components_pca['Coefficients'], axis='index'\n",
    "                    ).sum(axis=0).to_frame(name='Coefficients')\n",
    "\n",
    "# Source: https://stats.stackexchange.com/questions/223924/how-to-add-up-partial-confidence-intervals-to-create-a-total-confidence-interval\n",
    "stderror_pca = np.sqrt((X_train_pca_coeffs.multiply(stderror_components, axis='index')**2).sum(axis=0))\n",
    "\n",
    "df_importance_pca = plot_importance(df_importance_pca, title=\"Logistic Regression on PC's - Absolute Coefficient Values with 95% CI\",\n",
    "                                    error=1.96*stderror_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409cce9-d77c-4838-bfbc-196eb2785958",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2. Logistic Regression on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87cc85-225a-404b-afe6-337982bdd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove high-VIF columns\n",
    "\n",
    "# X_train_std = X_train_std[[col for col in X_train_std.columns if col not in high_vif_cols]]\n",
    "# X_test_std = X_test_std[[col for col in X_test_std.columns if col not in high_vif_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130d570-86b8-467c-ab15-0aec9fbc9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Hyperparameter tuning with Grid Search Cross Validation\n",
    "\n",
    "result_cv_logreg = gridsearch_cv(X_train_std, y_train, model=LogisticRegression(), \n",
    "                                 space=space_logreg, n_splits=4, n_repeats=2, refit='f1', random_state=0)\n",
    "result_cv_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2044856-3b92-4307-baf1-2db26d63aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_cv(result_cv_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f651d-2f5d-4353-98bf-dd8ccab52742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define best model\n",
    "model_logreg = LogisticRegression(\n",
    "    **result_cv_logreg.best_params_, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda8a75-1f03-41ab-bcb8-f91b14c2fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and make predictions\n",
    "model_logreg.fit(X_train_std, y_train)\n",
    "y_pred = model_logreg.predict(X_test_std)\n",
    "y_pred_proba = model_logreg.predict_proba(X_test_std)\n",
    "print(f'Accuracy on Training Set: {100*model_logreg.score(X_train_std, y_train):.1f}%')\n",
    "print(f'Accuracy on Test Set:     {100*model_logreg.score(X_test_std, y_test):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cba830-2d33-4a43-96dd-8d622b76dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33681a46-cbfb-4d34-9aff-3605f5241fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, target_names=[classes_dict[i] for i in model_logreg.classes_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9998261-e009-447b-a554-ba6942e9dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e043f76-4be9-4fb3-9196-6f376c0e9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.DataFrame(np.transpose(model_logreg.coef_), columns=['Coefficients'], index=X_train_std.columns)\n",
    "stderror = logit_stderror(model_logreg, X_train_std)[1:]\n",
    "df_importance = plot_importance(df_importance, title=\"Logistic Regression - Absolute Coefficient Values with 95% CI\",\n",
    "                                error=1.96*stderror)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25407596-c1dd-4158-9bed-f3b6ecc8d9b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb95d2f-f06f-496f-8a27-0ef966245dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625f89d-2f4c-41fb-a7d2-3e0b2312f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb2aaf-7928-433c-bef5-8e923d332f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_class_odds = y_train[y_train == 0].count() / y_train[y_train == 1].count()\n",
    "neg_class_odds = neg_class_odds.iloc[0]\n",
    "neg_class_odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5203d-c81c-417d-8317-f3fd9a9fd599",
   "metadata": {},
   "source": [
    "XGBoost parameter explanation: https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9a6fe-b722-4daa-88fd-f6c629d9d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_xgb = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'n_estimators': [40, 50, 60],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_child_weight': [2, 3, 4],\n",
    "    'gamma': [0, 0.25, 0.5],\n",
    "    'alpha':[0, 0.15, 0.3],\n",
    "    # ratio of number of negative class to the positive class (sum(negative instances) / sum(positive instances))\n",
    "    'scale_pos_weight': [neg_class_odds],\n",
    "    'eval_metric': ['logloss'],\n",
    "    'lambda':[1, 1.25],\n",
    "    ## 'subsample': [0.8, 1.0],\n",
    "    ## 'colsample_bytree': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e8511-55fd-4403-b0cc-1e0dd8b57550",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1. XGBoost on PC's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb0e98-f3aa-48c7-b9f4-93c05b0e2d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Hyperparameter tuning with Grid Search Cross Validation\n",
    "\n",
    "result_cv_xgb_pca = gridsearch_cv(X_train_pca, y_train, model=XGBClassifier(), \n",
    "                                 space=space_xgb, n_splits=4, n_repeats=1, refit='f1', random_state=0)\n",
    "result_cv_xgb_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932df7e4-e2fe-4b2a-a856-a1129c7bf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_cv(result_cv_xgb_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4293f2-fa00-426a-9727-4f61e8eb00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define best model\n",
    "model_xgb_pca = XGBClassifier(\n",
    "    **result_cv_xgb_pca.best_params_, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08258687-f310-4606-ba11-22fd2f72ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and make predictions\n",
    "model_xgb_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model_xgb_pca.predict(X_test_pca)\n",
    "y_pred_proba_pca = model_xgb_pca.predict_proba(X_test_pca)\n",
    "print(f'Accuracy on Training Set: {100*model_xgb_pca.score(X_train_pca, y_train):.1f}%')\n",
    "print(f'Accuracy on Test Set:     {100*model_xgb_pca.score(X_test_pca, y_test):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295a72c-5b1f-4d17-8b88-57003f0c5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b911342-33aa-4c23-b8e0-5ea61934104e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_pca = confusion_matrix(y_test, y_pred_pca)\n",
    "plot_confusion_matrix(cm_pca, target_names=[classes_dict[i] for i in model_xgb_pca.classes_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca9094-df00-4181-a08e-49f7f599a2e0",
   "metadata": {},
   "source": [
    "**SHAP Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d774d7a-6c2a-4d4c-a748-19ee95d11bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_xgb_pca)\n",
    "shap_values_pca = explainer.shap_values(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f0529-575c-4644-abf1-1fec7a3d0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Feature Importance', fontsize=BIGGER_SIZE)\n",
    "shap.summary_plot(shap_values_pca, X_test_pca, plot_type=\"bar\", class_names=[classes_dict[i] for i in model_xgb_pca.classes_],\n",
    "                  max_display=X_test_pca.shape[1], plot_size=(8,X_test_pca.shape[1]/2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd9128-15d3-40c1-b0b6-536f86b2f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap_pca = pd.DataFrame(shap_values_pca, columns=X_test_pca.columns)\n",
    "df_importance_pca = df_shap_pca.T.abs().mean(axis=1).to_frame(name='mean(|SHAP|)')\n",
    "\n",
    "# df_importance_pca.sort_values('mean(|SHAP|)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b130ef-44d8-4f2d-91e4-ca97510c2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = np.dot(shap_values_pca, X_train_pca_coeffs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf243f52-d45b-4934-9ffc-0bbd3ac90c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap = pd.DataFrame(shap_values, columns=X.columns)\n",
    "df_importance = df_shap.T.abs().mean(axis=1).to_frame(name='Feature Importance - derived from mean(|SHAP|)')\n",
    "\n",
    "# df_importance.sort_values('Feature Importance - derived from mean(|SHAP|)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf927db-7ca2-4b11-a28d-527ebbde3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Feature Importance - derived from PC's mean(|SHAP|)\", fontsize=BIGGER_SIZE)\n",
    "shap.summary_plot(shap_values, X_test_std, plot_type=\"bar\", class_names=[classes_dict[i] for i in model_xgb_pca.classes_], \n",
    "                  max_display=X_test_std.shape[1], plot_size=(12,X_test_std.shape[1]/2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0deeb-30e5-44e6-b4be-53f45c81b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(np.array(shap_values_pca), X_test_pca, feature_names=pca_cols, show=True, max_display=15, plot_size=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693da84-a9d7-448d-9c8e-6daca89885f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(np.array(shap_values), X_test_std, feature_names=X_test_std.columns, show=True, max_display=None, plot_size=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754d336-087e-4632-844e-28a5da9b003b",
   "metadata": {},
   "source": [
    "**Feature Importance from XGBoost 'gain' metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835bc930-ec57-4928-bb72-2b9ca75a415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_gain_pca = pd.DataFrame(model_xgb_pca.feature_importances_, index=pca_cols, columns=['PC Feature Gain'])\n",
    "df_importance_gain = X_train_pca_coeffs.multiply(df_importance_gain_pca['PC Feature Gain'], axis='index').sum(axis=0).abs().to_frame(name='Feature Gain')\n",
    "\n",
    "df_importance_gain.sort_values('Feature Gain', ascending=True).plot(kind=\"barh\", figsize=(12, X_test_std.shape[1]/2.5), legend=False)\n",
    "plt.gca().xaxis.grid(True)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.title(\"XGBoost - Original Feature's Importance\\n(derived from XGBoost 'gain' importance metric)\", pad=20, fontsize=BIGGER_SIZE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1c42d-1914-414c-935e-520b3b6d4360",
   "metadata": {},
   "source": [
    "### 4.2. XGBoost on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62614cd3-5748-4091-8b79-64830bbef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove high-VIF columns\n",
    "\n",
    "# X_train_std = X_train_std[[col for col in X_train_std.columns if col not in high_vif_cols]]\n",
    "# X_test_std = X_test_std[[col for col in X_test_std.columns if col not in high_vif_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf93242-d19e-4a9d-8578-924755081edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Hyperparameter tuning with Grid Search Cross Validation\n",
    "\n",
    "result_cv_xgb = gridsearch_cv(X_train_std, y_train, model=XGBClassifier(), \n",
    "                                 space=space_xgb, n_splits=4, n_repeats=1, refit='f1', random_state=0)\n",
    "result_cv_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771ae47-b3ab-4798-b267-a826cba14e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_cv(result_cv_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfc1e5-5af9-41a0-9ec2-a0d6c3f23c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define best model\n",
    "model_xgb = XGBClassifier(\n",
    "    **result_cv_xgb.best_params_, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7548b3-6f0a-4899-aeab-3d53b8572f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and make predictions\n",
    "model_xgb.fit(X_train_std, y_train)\n",
    "y_pred = model_xgb.predict(X_test_std)\n",
    "y_pred_proba = model_xgb.predict_proba(X_test_std)\n",
    "print(f'Accuracy on Training Set: {100*model_xgb.score(X_train_std, y_train):.1f}%')\n",
    "print(f'Accuracy on Test Set:     {100*model_xgb.score(X_test_std, y_test):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb92cc-bb29-4ed5-bd39-7ba4d1c476d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5416a07-f2af-40a0-b6b5-133355c6ce59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, target_names=[classes_dict[i] for i in model_xgb.classes_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65378e66-aab0-482e-bbcf-91a3f704a4ba",
   "metadata": {},
   "source": [
    "**SHAP Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6fe5b-8ea1-4d0a-aad9-a8dcc3844a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_xgb)\n",
    "shap_values = explainer.shap_values(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5506ff-98a2-4df2-b0c7-61125f0dcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Feature Importance', fontsize=BIGGER_SIZE)\n",
    "shap.summary_plot(shap_values, X_test_std, plot_type=\"bar\", class_names=[classes_dict[i] for i in model_xgb_pca.classes_], \n",
    "                  max_display=X_test_std.shape[1], plot_size=(12,X_test_std.shape[1]/2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3513c-60dc-49a0-93c1-eb983dcc82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap = pd.DataFrame(shap_values, columns=X_test_std.columns)\n",
    "df_importance = df_shap.T.abs().mean(axis=1).to_frame(name='mean(|SHAP|)')\n",
    "\n",
    "# df_importance.sort_values('mean(|SHAP|)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e87e4-7734-4417-a9db-b2edaed9f734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(np.array(shap_values), X_test_std, feature_names=X_test_std.columns, show=True, max_display=X_test_std.shape[1], plot_size=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06abe44-6b0a-489f-8ebf-ebd2c708ed46",
   "metadata": {},
   "source": [
    "**Feature Importance from XGBoost 'gain' metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f0371-c95a-4499-932d-1da54adcc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_gain = pd.DataFrame(model_xgb.feature_importances_, index=X_test_std.columns, columns=['Feature Gain'])\n",
    "\n",
    "df_importance_gain.sort_values('Feature Gain', ascending=True).plot(kind=\"barh\", figsize=(12, X_test_std.shape[1]/2.5), legend=False)\n",
    "plt.gca().xaxis.grid(True)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.title(\"XGBoost - Feature Importance\\n(derived from XGBoost 'gain' importance metric)\", pad=20, fontsize=BIGGER_SIZE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4a479-04b6-442f-9fa0-7a220e6a368e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c68430-8e8d-4e3a-825b-1b48f64341fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
